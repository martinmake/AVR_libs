#include "layers/relu.h"
#include "cuda/debug.cuh"
#include "cuda/std.cuh"

namespace Anna
{
	namespace Layer
	{
		__global__ static void cuda_activate_kernel(float* data, uint64_t count)
		{
			uint16_t idx = threadIdx.x +
			               blockIdx.x * blockDim.x;

			if (idx < count)
			{
				if (data[idx] < 0)
					data[idx] == 0;
			}
		}
		void Relu::activate(void)
		{
			uint64_t count = m_shape.hypervolume();

			#ifdef USE_CUDA
				dim3 block(count < 1024 ? count : 1024);
				dim3 grid((count + block.x - 1) / block.x);

				cuda_activate_kernel<<<grid, block>>>(m_output.data(), count);
				cudaCall(cudaDeviceSynchronize());
			#else
				// TODO: CPU
			#endif
		}
	}
}
